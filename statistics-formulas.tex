\documentclass[landscape]{article}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{pgfplots}
\usepackage{hyperref}

\title{Statistic Formulas}
\author{RobinXSI}
\date{Juli 2015}
\begin{document}
   \maketitle
   	\section{Introduction}

   		Wichtig beim aufstellen von Hypothesen ist, dass die Auswertung eine statistische Signifikanz haben.

   		Statistical significance means:
		\begin{itemize}
			\item rected the null hypothesis
			\item results are not likely due to chance (sampling error)
		\end{itemize}

		Als Endresultat der Statistik sollten folgendes Dokument erstellt werden können:
		\begin{itemize}
			\item Desciriptive Statistics
				\begin{itemize}
					\item M - Mean
					\item Sd - Standard Deviation
				\end{itemize}
			\item Inferential Statistics
				\begin{itemize}
					\item Hypothesis Test \(\alpha\)
					\begin{itemize}
						\item kind of test, bsp: one-sample t-test
						\item test statistic, bsp: t-value
						\item df - degrees of freedom
						\item p-value
						\item direction of test, bsp: one-tail-test or two-tail-test
					\end{itemize}
					\item APA style
						  \\\(t(df) = X.XX, p = X.XX,\) direction
						  \\bsp: \(t(24) = -2.50, p < .05,\) one-tailed
					\item Confidence intervals
					\begin{itemize}
						\item Confidence leve, bsp: 95\%
						\item Lower Limit
						\item Upper Limit
						\item CI on what?
					\end{itemize}
					\item APA style - CIs
						  \\Confidence interval on the mean difference;
						  \\95\% CI = (4 to 6)
					\item Effect size measures
						\begin{itemize}
							\item \(d\)
							\item \(r^2\)
						\end{itemize}
					\item APA style - CIs
						  \\\(d = X.XX\)
						  \\\(r^2 = .XX\)

				\end{itemize}
		\end{itemize}


	\section{Statistical Formulas}
		\pgfmathdeclarefunction{gauss}{2}{%
		  \pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
		}

		\begin{tikzpicture}
			\begin{axis}[
			  no markers, domain=0:10, samples=100,
			  axis lines*=left, xlabel=$x$, ylabel=$y$,
			  every axis y label/.style={at=(current axis.above origin),anchor=south},
			  every axis x label/.style={at=(current axis.right of origin),anchor=west},
			  height=5cm, width=15cm,
			  xtick={4}, ytick=\empty,
			  enlargelimits=false, clip=false, axis on top,
			  grid = major
			  ]
			  \addplot [fill=cyan!20, draw=none, domain=0:2.4] {gauss(4,1)} \closedcycle;
			  \addplot [fill=cyan!20, draw=none, domain=5.6:10] {gauss(4,1)} \closedcycle;
			  \addplot [very thick,cyan!50!black] {gauss(4,1)};
			 


			\draw [yshift=-0.6cm, latex-latex](axis cs:4,0) -- node [fill=white] {$1.96\sigma$} (axis cs:5.6,0);
			\end{axis}

		\end{tikzpicture}

		\subsection{Empirical Mean}
		\(\gamma = \frac{\sum{x}}{N}\)

		Properties (for independent random variables X and Y):
		\begin{enumerate}
			\item \(Mean(X + Y) = Mean(X) + Mean(Y)\)
			\item \(Mean(X \times Y) = Mean(X) \times Mean(Y)\)
		\end{enumerate}

		\subsection{Variance}
		\(\sigma^2 = Var(X)\newline
			\sigma^2 = \frac{\sum{(x_i - \gamma)^2}}{N}\newline
			\sigma^2 = \frac{\sum{X_i^2}}{N} - \frac{(\sum{X_i})^2}{N^2}\newline
			\sigma^2 = \frac{\sum{(x_i - \gamma)^2}}{N}\)\newline
		
		\subsection{Standard Deviation}
		\(\sigma = \sqrt{Var(X)}\newline
			\sigma = \sqrt{\frac{\sum{(X_i - \gamma)^2}}{N}}\)\newline

		\subsection{Standard Error}
		\(Sd = \frac{\sigma}{N}\)

		\subsection{Z-Score}
		Z-Score \(= \frac{x - \bar{x}}{Sd}\)

		\subsection{Standard Normal Distribution}
		\(\frac{1}{\sqrt{2 \cdot \pi \cdot \sigma^2}} \cdot e^{[-\frac{1}{2} \cdot \frac{(x-\gamma)^2}{\sigma^2}]}\)

		\subsection{Confidence Interval}
		\(CI = 1.96 \cdot \sqrt{\frac{p(1 - p)}{N}}\newline
			General Form:\newline
			Size of CI = a \cdot \sqrt{\frac{\sigma^2}{N}}\newline
			\frac{\sum{X_i \pm a \cdot \sqrt{\frac{\sigma^2}{N}}}}{N}\)

			\textbf{Note}

			\begin{enumerate}
			\item $a = 1.96 for N \geq 30$
			\item $a$ is the t-value computed for $(N - 1)$ degrees of freedom and confidence level $p$.
			\end{enumerate}

		\subsection{Additional Formulas}
		\begin{tabular}{lllll}
			\hline
			Name & Population Symbol & Sample Symbol & Sample Calculation & Beschreibung \\ \hline
			Mean &  & $\bar{x}$ & $\bar{x} = \frac{\sum{x}}{N}$ & Durchschnitt aller Daten \\
			Variance & $\sigma_x^2$ & $s_x^2$ & $s_x^2 = \frac{\sum{(x - \bar{x})^2}}{N - 1}$ & Abweichung \\
			Standard Dev & $\sigma_x$ & $s_x$ & $s_x = \sqrt{s_x^2}$ &  \\
			Covariance & $\sigma_xy$ & $s_xy$ & $s_xy = \frac{\sum{(x - \bar{x})(y - \bar{y})}}{N - 1}$ &  \\
			Correlation & $\rho_xy$ & $r_xy$ & \begin{tabular}[c]{@{}l@{}}$r_xy = \frac{s_xy}{s_x s_y}$\\ $r_xy = \frac{\sum{(z_x z_y)}}{N - 1}$\end{tabular} &  \\ \hline
			z-score & $z_x$ & $z_x$ & $z_x = \frac{x-\bar{x}}{s_x}; \bar{z} = 0; s_x^2 = 1$ &  \\ \hline
		\end{tabular}

	\subsection{T-Test}
		\[
		t = \frac{\bar{x_D} - 0}{s_D / \sqrt{n}}
		\]

	


	\section{Lesson 10 - Dependent samples}

		Dependent samples (repeated measures)
		\begin{itemize}
			\item Two Conditions
			\item Longitudinal
			\item Pre-Test, Post-Test
		\end{itemize}

		\subsection{Important formulas}
			\(x = [ARRAY of VALUES] \Rightarrow\) Population Data
			\(X = [ARRAY of VALUES] \Rightarrow\) Sample Data
			\\\(n \Rightarrow\) Sample und Population Size
			\\\(\mu = \frac{\sum{x_i}}{n}\Rightarrow\) Population Mean
			\\\(\bar{X} = \frac{\sum{X_i}}{n} \Rightarrow\) Sample Mean
			\\\(s_D = \sqrt{\frac{\sum{(X_i - \mu)^2}}{n}} \Rightarrow\) Standard Deviation for sample
			\\\(\alpha \Rightarrow\) tail probability on t-table
			\\\(t_{critical} \Rightarrow\) from \href{https://s3.amazonaws.com/udacity-hosted-downloads/t-table.jpg}{t-table}
			\\\(df = n - 1 \Rightarrow\) Degrees of Freedom
			\\\(SEM = \frac{s_D}{\sqrt{n}} \Rightarrow\) Standard Error of the Mean
			\\\(t = \frac{\bar{X} - \mu}{SEM} \Rightarrow\) One Sample t-Test
			\\margin of error \(= (t^{critical} \times SEM)\)
			\\\(CI = \bar{X} \pm \) margin of error \(\Rightarrow\) Confidence Interval
			\\\(d = \frac{\bar{X}-\mu}{s_D} \Rightarrow\) Cohen's d \(\Rightarrow\) Standardized mean difference 
			\\\(r^2 = \frac{t^2}{t^2 + df} \Rightarrow\) bestimmt die Stärke des Zusammenhangs zwischen zwei Variablen als Proportion. Beispiel: \(r^2 gibt\) an wieviel das Geschlecht einer Person zum Unterschied der beiden Samples beigetragen hat

		\subsection{Hypothese}
			``US families spent an average of \$151 per week on food''
			\\Beispiel Null Hypothese: ``the program did not change the cost of food''
			\\Beispiel Alternative Hypothese: ``the program reduced the cost of food''
			\\
			\\\(null \rightarrow H_0:\mu_program >= 151\)
			\\\(alt \rightarrow H_A:\mu_program < 151\)

	\section{Lesson 11 - Independent Samples}
		Voneinander abhängige Daten brauchen nicht so viele Testkandidaten, ist kosteneffektiv und weniger zeitaufwendig. Diese Art an Daten zu kommen hat aber auch Nachteile. Beispielsweise könnten die Probanden beim zweiten Ausfüllen eines Tests die Antworten schon kennen.

		Aus diesem Grund braucht es Independent Samples, also je einen Teil des Samples, der das Treatment gemacht hat und einen anderen Teil der es nicht gemacht hat.
		\\\(s_1 \sqrt{\frac{\sum{(X_i1 - \mu)^2}}{n - 1}} \Rightarrow\) Standard Deviation for sample with
		 \href{Bessel\'s correction}{http://www.wikiwand.com/en/Bessel\%27s\_correction}
		\\\(s_2 \sqrt{\frac{\sum{(X_i2 - \mu)^2}}{n - 1}} \Rightarrow\) Standard Deviation for sample with \href{Bessel\'s correction}{http://www.wikiwand.com/en/Bessel\%27s\_correction}
		\\\(n_1 \Rightarrow\) Size of Sample 1
		\\\(n_2 \Rightarrow\) Size of Sample 2
		\\\(S_D = \sqrt{s_1^2 + s_2^2}\) Standard Deviation for Samples
		\\\(SEM = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}} \Rightarrow\) Standard Error for independent samples
		\\\(df = n_1 + n_2 - 2 \Rightarrow\) Degrees of freedom for independent samples
		\\\(t = \frac{\bar{X}_1 - \bar{X}_2}{SEM} \Rightarrow\) Two Sample t-Test
		\\\(CI = (\bar{X}_1 - \bar{X}_2) \pm \) margin of error \(\Rightarrow\) Confidence Interval
	

\end{document}